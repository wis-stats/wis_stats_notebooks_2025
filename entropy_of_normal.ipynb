{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e684cc37-ea93-4fe0-8c48-0f99ef995a78",
      "metadata": {},
      "source": [
        "# Entropy of the Normal distribution {#exr-normal-entropy}\n",
        "\n",
        "Consider a univariate Normal distribution with location parameter $\\mu$ and scale parameter $\\sigma$. Because it is continuous, we cannot define an entropy, but instead can us the analogous concept of a **differential entropy**, which is defined for a univariate continuous distribution as\n",
        "\n",
        "$$\\begin{align}\n",
        "h[f(y)] = -\\int\\mathrm{d}y\\,f(y)\\,\\ln f(y),\n",
        "\\end{align}\n",
        "$${#eq-differential_entropy}\n",
        "\n",
        "though the choice of the base of the logarithm is arbitrary.\n",
        "\n",
        "**a)** Just using your intuition, do you think the entropy of the Normal distribution is a function on $\\mu$?\n",
        "\n",
        "**b)** Compute the entropy of a Normal distribution. (Hint: It may help to look at [Gaussian integrals](https://en.wikipedia.org/wiki/Gaussian_integral). But you do not have to explicitly integrate if you recall the moments of a Normal distribution, which, admittedly, are themselves computed by evaluating Gaussian integrals.) Was your intuition correct?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}